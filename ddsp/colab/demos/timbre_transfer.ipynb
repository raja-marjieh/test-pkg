{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3YLyiTwPfVCT"
      },
      "source": [
        "\u003ca href=\"https://colab.research.google.com/github/magenta/ddsp/blob/master/ddsp/colab/demos/timbre_transfer.ipynb\" target=\"_parent\"\u003e\u003cimg src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/\u003e\u003c/a\u003e\n",
        "\n",
        "##### Copyright 2020 Google LLC.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Bvp6GWqtfVCW"
      },
      "outputs": [],
      "source": [
        "# Copyright 2020 Google LLC. All Rights Reserved.\n",
        "\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JndnmDMp66FL"
      },
      "source": [
        "# DDSP Timbre Transfer Demo\n",
        "\n",
        "This notebook is a demo of timbre transfer using DDSP (Differentiable Digital Signal Processing). \n",
        "The model here is trained to generate audio conditioned on a time series of fundamental frequency and loudness. \n",
        "\n",
        "* [DDSP ICLR paper](https://openreview.net/forum?id=B1x1ma4tDr)\n",
        "* [Audio Examples](http://goo.gl/magenta/ddsp-examples) \n",
        "\n",
        "By default, the notebook will download pre-trained models for Violin and Flute. You can train a model on your own sounds by using the [Train Autoencoder Colab](https://github.com/magenta/ddsp/blob/master/ddsp/colab/demos/train_autoencoder.ipynb).\n",
        "\n",
        "\u003cimg src=\"https://storage.googleapis.com/ddsp/additive_diagram/ddsp_autoencoder.png\" alt=\"DDSP Autoencoder figure\" width=\"700\"\u003e\n",
        "\n",
        "\n",
        "# Environment Setup\n",
        "\n",
        "\n",
        "This notebook extracts these features from input audio (either uploaded files, or recorded from the microphone) and resynthesizes with the model.\n",
        "\n",
        "Have fun! And please feel free to hack this notebook to make your own creative interactions.\n",
        "\n",
        "### Instructions for running:\n",
        "\n",
        "* Make sure to use a GPU runtime, click:  __Runtime \u003e\u003e Change Runtime Type \u003e\u003e GPU__\n",
        "* Press the ▶️button on the left of each of the cells\n",
        "* View the code: Double-click any of the cells\n",
        "* Hide the code: Double click the right side of the cell\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "6wZde6CBya9k"
      },
      "outputs": [],
      "source": [
        "#@title #Install and Import\n",
        "\n",
        "#@markdown Install ddsp, define some helper functions, and download the model. This transfers a lot of data and _should take a minute or two_.\n",
        "%tensorflow_version 2.x\n",
        "\n",
        "print('Installing from pip package...')\n",
        "!pip install -qU ddsp\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "# Ignore a bunch of deprecation warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import copy\n",
        "import os\n",
        "import time\n",
        "\n",
        "import crepe\n",
        "import ddsp\n",
        "import ddsp.training\n",
        "from ddsp.colab.colab_utils import (download, play, record, specplot, upload,\n",
        "                                    DEFAULT_SAMPLE_RATE)\n",
        "import gin\n",
        "from google.colab import files\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow.compat.v2 as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Helper Functions\n",
        "sample_rate = DEFAULT_SAMPLE_RATE  # 16000\n",
        "\n",
        "\n",
        "print('Done!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "Go36QW9AS_CD"
      },
      "outputs": [],
      "source": [
        "#@title Record or Upload Audio\n",
        "#@markdown * Either record audio from microphone or upload audio from file (.mp3 or .wav) \n",
        "#@markdown * Audio should be monophonic (single instrument / voice)\n",
        "#@markdown * Extracts fundmanetal frequency (f0) and loudness features. \n",
        "\n",
        "record_or_upload = \"Record\"  #@param [\"Record\", \"Upload (.mp3 or .wav)\"]\n",
        "\n",
        "record_seconds =   5  #@param {type:\"number\", min:1, max:10, step:1}\n",
        "\n",
        "if record_or_upload == \"Record\":\n",
        "  audio = record(seconds=record_seconds)\n",
        "else:\n",
        "  # Load audio sample here (.mp3 or .wav3 file)\n",
        "  # Just use the first file.\n",
        "  filenames, audios = upload()\n",
        "  audio = audios[0]\n",
        "audio = audio[np.newaxis, :]\n",
        "print('\\nExtracting audio features...')\n",
        "\n",
        "# Plot.\n",
        "specplot(audio)\n",
        "play(audio)\n",
        "\n",
        "# Setup the session.\n",
        "ddsp.spectral_ops.reset_crepe()\n",
        "\n",
        "# Compute features.\n",
        "start_time = time.time()\n",
        "audio_features = ddsp.training.eval_util.compute_audio_features(audio)\n",
        "audio_features['loudness_db'] = audio_features['loudness_db'].astype(np.float32)\n",
        "audio_features_mod = None\n",
        "print('Audio features took %.1f seconds' % (time.time() - start_time))\n",
        "\n",
        "\n",
        "# Plot Features.\n",
        "fig, ax = plt.subplots(nrows=3, \n",
        "                       ncols=1, \n",
        "                       sharex=True,\n",
        "                       figsize=(6, 8))\n",
        "ax[0].plot(audio_features['loudness_db'])\n",
        "ax[0].set_ylabel('loudness_db')\n",
        "\n",
        "ax[1].plot(librosa.hz_to_midi(audio_features['f0_hz']))\n",
        "ax[1].set_ylabel('f0 [midi]')\n",
        "\n",
        "ax[2].plot(audio_features['f0_confidence'])\n",
        "ax[2].set_ylabel('f0 confidence')\n",
        "_ = ax[2].set_xlabel('Time step [frame]')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "wmSGDWM5yyjm"
      },
      "outputs": [],
      "source": [
        "#@title Choose a model\n",
        "\n",
        "model = 'Violin' #@param ['Violin', 'Flute', 'Flute2', 'Trumpet', 'Tenor_Saxophone','Upload your own (checkpoint folder as .zip)']\n",
        "MODEL = model\n",
        "\n",
        "\n",
        "def find_model_dir(dir_name):\n",
        "  # Iterate through directories until model directory is found\n",
        "  for root, dirs, filenames in os.walk(dir_name):\n",
        "    for filename in filenames:\n",
        "      if filename.endswith(\".gin\") and not filename.startswith(\".\"):\n",
        "        model_dir = root\n",
        "        break\n",
        "  return model_dir \n",
        "\n",
        "\n",
        "if model in ('Violin', 'Flute', 'Flute2', 'Trumpet', 'Tenor_Saxophone'):\n",
        "  # Pretrained models.\n",
        "  PRETRAINED_DIR = '/content/pretrained'\n",
        "  # Copy over from gs:// for faster loading.\n",
        "  !rm -r $PRETRAINED_DIR \u0026\u003e /dev/null\n",
        "  !mkdir $PRETRAINED_DIR \u0026\u003e /dev/null\n",
        "  GCS_CKPT_DIR = 'gs://ddsp/models/tf2'\n",
        "  model_dir = os.path.join(GCS_CKPT_DIR, 'solo_%s_ckpt' % model.lower())\n",
        "  \n",
        "  !gsutil cp $model_dir/* $PRETRAINED_DIR \u0026\u003e /dev/null\n",
        "  model_dir = PRETRAINED_DIR\n",
        "  gin_file = os.path.join(model_dir, 'operative_config-0.gin')\n",
        "\n",
        "else:\n",
        "  # User models.\n",
        "  UPLOAD_DIR = '/content/uploaded'\n",
        "  !mkdir $UPLOAD_DIR\n",
        "  uploaded_files = files.upload()\n",
        "\n",
        "  for fnames in uploaded_files.keys():\n",
        "    print(\"Unzipping... {}\".format(fnames))\n",
        "    !unzip -o \"/content/$fnames\" -d $UPLOAD_DIR \u0026\u003e /dev/null\n",
        "  model_dir = find_model_dir(UPLOAD_DIR)\n",
        "  gin_file = os.path.join(model_dir, 'operative_config-0.gin')\n",
        "\n",
        "# Parse gin config,\n",
        "with gin.unlock_config():\n",
        "  gin.parse_config_file(gin_file, skip_unknown=True)\n",
        "\n",
        "# Assumes only one checkpoint in the folder, 'ckpt-[iter]`.\n",
        "ckpt_files = [f for f in tf.io.gfile.listdir(model_dir) if 'ckpt' in f]\n",
        "ckpt_name = ckpt_files[0].split('.')[0]\n",
        "ckpt = os.path.join(model_dir, ckpt_name)\n",
        "\n",
        "# Ensure dimensions and sampling rates are equal\n",
        "time_steps_train = gin.query_parameter('DefaultPreprocessor.time_steps')\n",
        "n_samples_train = gin.query_parameter('Additive.n_samples')\n",
        "hop_size = int(n_samples_train / time_steps_train)\n",
        "\n",
        "time_steps = int(audio.shape[1] / hop_size)\n",
        "n_samples = time_steps * hop_size\n",
        "\n",
        "# print(\"===Trained model===\")\n",
        "# print(\"Time Steps\", time_steps_train)\n",
        "# print(\"Samples\", n_samples_train)\n",
        "# print(\"Hop Size\", hop_size)\n",
        "# print(\"\\n===Resynthesis===\")\n",
        "# print(\"Time Steps\", time_steps)\n",
        "# print(\"Samples\", n_samples)\n",
        "# print('')\n",
        "\n",
        "gin_params = [\n",
        "    'RnnFcDecoder.input_keys = (\"f0_scaled\", \"ld_scaled\")',\n",
        "    'Additive.n_samples = {}'.format(n_samples),\n",
        "    'FilteredNoise.n_samples = {}'.format(n_samples),\n",
        "    'DefaultPreprocessor.time_steps = {}'.format(time_steps),\n",
        "]\n",
        "\n",
        "with gin.unlock_config():\n",
        "  gin.parse_config(gin_params)\n",
        "\n",
        "\n",
        "# Trim all input vectors to correct lengths \n",
        "for key in ['f0_hz', 'f0_confidence', 'loudness_db']:\n",
        "  audio_features[key] = audio_features[key][:time_steps]\n",
        "audio_features['audio'] = audio_features['audio'][:, :n_samples]\n",
        "\n",
        "\n",
        "# Set up the model just to predict audio given new conditioning\n",
        "model = ddsp.training.models.Autoencoder()\n",
        "model.restore(ckpt)\n",
        "\n",
        "# Build model by running a batch through it.\n",
        "start_time = time.time()\n",
        "_ = model(audio_features, training=False)\n",
        "print('Restoring model took %.1f seconds' % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "uQFUlIJ_5r36"
      },
      "outputs": [],
      "source": [
        "#@title Modify conditioning\n",
        "\n",
        "#@markdown These models were not explicitly trained to perform timbre transfer, so they may sound unnatural if the incoming loudness and frequencies are very different then the training data (which will always be somewhat true). \n",
        "\n",
        "#@markdown This button will at least adjusts the average loudness and pitch to be similar to the training data (although not for user trained models).\n",
        "\n",
        "auto_adjust = True #@param{type:\"boolean\"}\n",
        "\n",
        "#@markdown You can also make additional manual adjustments:\n",
        "#@markdown * Shift the fundmental frequency to a more natural register.\n",
        "#@markdown * Silence audio below a threshold on f0_confidence.\n",
        "#@markdown * Adjsut the overall loudness level.\n",
        "f0_octave_shift =  0 #@param {type:\"slider\", min:-2, max:2, step:1}\n",
        "f0_confidence_threshold =  0 #@param {type:\"slider\", min:0.0, max:1.0, step:0.05}\n",
        "loudness_db_shift = 0 #@param {type:\"slider\", min:-20, max:20, step:1}\n",
        "\n",
        "#@markdown You might get more realistic sounds by shifting a few dB, or try going extreme and see what weird sounds you can make...\n",
        "\n",
        "audio_features_mod = {k: v.copy() for k, v in audio_features.items()}\n",
        "\n",
        "\n",
        "## Helper functions.\n",
        "def shift_ld(audio_features, ld_shift=0.0):\n",
        "  \"\"\"Shift loudness by a number of ocatves.\"\"\"\n",
        "  audio_features['loudness_db'] += ld_shift\n",
        "  return audio_features\n",
        "\n",
        "\n",
        "def shift_f0(audio_features, f0_octave_shift=0.0):\n",
        "  \"\"\"Shift f0 by a number of ocatves.\"\"\"\n",
        "  audio_features['f0_hz'] *= 2.0 ** (f0_octave_shift)\n",
        "  audio_features['f0_hz'] = np.clip(audio_features['f0_hz'], \n",
        "                                    0.0, \n",
        "                                    librosa.midi_to_hz(110.0))\n",
        "  return audio_features\n",
        "\n",
        "\n",
        "def mask_by_confidence(audio_features, confidence_level=0.1):\n",
        "  \"\"\"For the violin model, the masking causes fast dips in loudness. \n",
        "  This quick transient is interpreted by the model as the \"plunk\" sound.\n",
        "  \"\"\"\n",
        "  mask_idx = audio_features['f0_confidence'] \u003c confidence_level\n",
        "  audio_features['f0_hz'][mask_idx] = 0.0\n",
        "  # audio_features['loudness_db'][mask_idx] = -ddsp.spectral_ops.LD_RANGE\n",
        "  return audio_features\n",
        "\n",
        "\n",
        "def smooth_loudness(audio_features, filter_size=3):\n",
        "  \"\"\"Smooth loudness with a box filter.\"\"\"\n",
        "  smoothing_filter = np.ones([filter_size]) / float(filter_size)\n",
        "  audio_features['loudness_db'] = np.convolve(audio_features['loudness_db'], \n",
        "                                           smoothing_filter, \n",
        "                                           mode='same')\n",
        "  return audio_features\n",
        "\n",
        "if auto_adjust:\n",
        "  if MODEL in ['Violin', 'Flute', 'Flute2', 'Trumpet', 'Saxophone', 'Tenor_Saxophone']:\n",
        "    # Adjust the peak loudness.\n",
        "    l = audio_features['loudness_db']\n",
        "    model_ld_avg_max = {\n",
        "        'Violin': -34.0,\n",
        "        'Flute': -45.0,\n",
        "        'Flute2': -44.0,\n",
        "        'Trumpet': -52.3,\n",
        "        'Tenor_Saxophone': -31.2\n",
        "    }[MODEL]\n",
        "    ld_max = np.max(audio_features['loudness_db'])\n",
        "    ld_diff_max = model_ld_avg_max - ld_max\n",
        "    audio_features_mod = shift_ld(audio_features_mod, ld_diff_max)\n",
        "\n",
        "    # Further adjust the average loudness above a threshold.\n",
        "    l = audio_features_mod['loudness_db']\n",
        "    model_ld_mean = {\n",
        "        'Violin': -44.0,\n",
        "        'Flute': -51.0,\n",
        "        'Flute2': -53.0,\n",
        "        'Trumpet': -69.2,\n",
        "        'Tenor_Saxophone': -50.8\n",
        "    }[MODEL]\n",
        "    ld_thresh = -70.0\n",
        "    ld_mean = np.mean(l[l \u003e ld_thresh])\n",
        "    ld_diff_mean = model_ld_mean - ld_mean\n",
        "    audio_features_mod = shift_ld(audio_features_mod, ld_diff_mean)\n",
        "\n",
        "    # Shift the pitch register.\n",
        "    model_p_mean = {\n",
        "        'Violin': 73.0,\n",
        "        'Flute': 81.0,\n",
        "        'Flute2': 74.0,\n",
        "        'Trumpet': 65.8,\n",
        "        'Tenor_Saxophone': 57.8\n",
        "    }[MODEL]\n",
        "    p = librosa.hz_to_midi(audio_features['f0_hz'])\n",
        "    p[p == -np.inf] = 0.0\n",
        "    p_mean = p[l \u003e ld_thresh].mean()\n",
        "    p_diff = model_p_mean - p_mean\n",
        "    p_diff_octave = p_diff / 12.0\n",
        "    round_fn = np.floor if p_diff_octave \u003e 1.5 else np.ceil\n",
        "    p_diff_octave = round_fn(p_diff_octave)\n",
        "    audio_features_mod = shift_f0(audio_features_mod, p_diff_octave)\n",
        "\n",
        "  else:\n",
        "    print('\\nUser uploaded model: disabling auto-adjust.')\n",
        "\n",
        "  \n",
        "audio_features_mod = shift_ld(audio_features_mod, loudness_db_shift)\n",
        "audio_features_mod = shift_f0(audio_features_mod, f0_octave_shift)\n",
        "audio_features_mod = mask_by_confidence(audio_features_mod, f0_confidence_threshold)\n",
        "\n",
        "\n",
        "# Plot Features.\n",
        "fig, ax = plt.subplots(nrows=3, \n",
        "                       ncols=1, \n",
        "                       sharex=True,\n",
        "                       figsize=(6, 8))\n",
        "ax[0].plot(audio_features['loudness_db'])\n",
        "ax[0].plot(audio_features_mod['loudness_db'])\n",
        "ax[0].set_ylabel('loudness_db')\n",
        "ax[0].legend(['Original','Adjusted'])\n",
        "\n",
        "ax[1].plot(librosa.hz_to_midi(audio_features['f0_hz']))\n",
        "ax[1].plot(librosa.hz_to_midi(audio_features_mod['f0_hz']))\n",
        "ax[1].set_ylabel('f0 [midi]')\n",
        "ax[1].legend(['Original','Adjusted'])\n",
        "\n",
        "ax[2].plot(audio_features_mod['f0_confidence'])\n",
        "ax[2].plot(np.ones_like(audio_features_mod['f0_confidence']) * f0_confidence_threshold)\n",
        "ax[2].set_ylabel('f0 confidence')\n",
        "_ = ax[2].set_xlabel('Time step [frame]')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "SLwg1WkHCXQO"
      },
      "outputs": [],
      "source": [
        "#@title #Resynthesize Audio\n",
        "\n",
        "af = audio_features if audio_features_mod is None else audio_features_mod\n",
        "\n",
        "# Run a batch of predictions.\n",
        "start_time = time.time()\n",
        "audio_gen = model(af, training=False)\n",
        "print('Prediction took %.1f seconds' % (time.time() - start_time))\n",
        "\n",
        "# Plot\n",
        "print('Original')\n",
        "play(audio)\n",
        "\n",
        "print('Resynthesis')\n",
        "play(audio_gen)\n",
        "\n",
        "specplot(audio)\n",
        "plt.title(\"Original\")\n",
        "\n",
        "specplot(audio_gen)\n",
        "_ = plt.title(\"Resynthesis\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "3YLyiTwPfVCT"
      ],
      "last_runtime": {},
      "name": "timbre_transfer.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
